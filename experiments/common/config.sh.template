#!/usr/bin/env bash

# AWS specific values
SSH_KEY_NAME="<YOUR_AWS_KEY_NAME>"  # The name of the AWS key used to access S3 and EC2
INSTANCE_PROFILE="<YOUR_PREFERRED_ROLE>"  # Identifies an IAM to pass to created EC2 instances

S3_REGION=""  # The S3 region; e.g. "eu-central-1"
S3_INPUT_BUCKET=""  # The S3 bucket name where datasets are stored without the 's3://' prefix; e.g. "my-new-bucket"
S3_INPUT_PATH=s3://${S3_INPUT_BUCKET}  # The full path within the bucket where the data is stored; e.g. s3://my-new-bucket/path/to/folder

# Create a user with read-only access to S3 or just the bucket above and
# create an access key for that user as described in this guide:
# https://aws.amazon.com/premiumsupport/knowledge-center/create-access-key/
# Then paste the key components into the variables below. This is required only
# by the experiments with RDataFrames.
S3_SECRET_KEY='...'
S3_ACCESS_KEY='...'

# GCP specific values
GS_REGION=""  # The GCS region; e.g. "europe-west4"
GS_INPUT_BUCKET=""  # The GCS bucket name where datasets are stored without the 'gs://' prefix; e.g. "my-new-bucket"
GS_INPUT_PATH=gs://${GS_INPUT_BUCKET}  # The full path within the bucket where the data is stored; e.g. gs://my-new-bucket/path/to/folder
GS_DATASET_ID='iris_hep_benchmark_data' # "namespace" for BigQuery tables. This value should work fine.
